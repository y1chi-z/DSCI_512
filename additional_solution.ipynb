{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe8385e9-b34a-43c0-b516-ca39c5d61ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.request\n",
    "from collections import defaultdict, Counter\n",
    "%config InlineBackend.figure_formats = ['svg']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09790f8-0b75-421e-bf63-e48f9b3515b0",
   "metadata": {},
   "source": [
    "## Exercise 6: Markov Model of language (optional)\n",
    "\n",
    "_Meta-comment 1_: this is more of a programming exercise. There was some talk of having it in DSCI 511, but we ended up putting it here. However, it's definitely good practice in _using_ Python data structures like dictionaries. There are some more challenging questions about time/space complexity at the end, which you can skip if you don't have enough time for them. Overall, this is not a perfect thematic fit with DSCI 512, but it's very good practice (and hopefully fun!)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f194338-5163-4305-8d64-806f4ebf64a4",
   "metadata": {},
   "source": [
    "In this exercise we will try to synthesize English text by \"learning\" from some input text, also known as a _corpus_. As an example, let's say the input text is the following, taken from the MDS website:\n",
    "\n",
    "> Data is everywhere. Continuously generated and collected across every domain, it is a vast and largely untapped resource of information with the potential to reveal insights about every aspect of our lives and the world we live in. However, the ability to uncover these insights is a highly specialized skill possessed by far too few. \n",
    "\n",
    "Our algorithm involves a parameter, which we'll call $n$. Let me first explain the approach when $n=1$: \n",
    "\n",
    "- We will start with an initial character, say \"y\". There are 8 occurrences of \"y\" in the input text above. What character typically comes after \"y\"? It turns out (according to the input text above) the next letter is \"w\" the first time and \" \" (space character) all the other 7 times. So we estimate the conditional probability distribution of the next character, given that the current character is \"y\", to be:\n",
    "    - P(\"w\" after \"y\") = 1/8 (_the probability that \"w\" comes after \"y\" is 1/8_)\n",
    "    - P(\" \" after \"y\") = 7/8\n",
    "    - probability zero for all other characters\n",
    "- To generate the next character, we generate a sample from this simple distribution. Say we pick \" \", so we add a \" \" to our output text and it is now \"y \". Now \" \" is our current character. To generate the next character, we'd need to probability distribution of what comes after \" \" so that we could sample from it. We'd repeat this until the output text reaches a pre-specified length.\n",
    "\n",
    "What about larger $n$? For $n=3$, we pick the next character by looking at the _preceding 3 characters_. We use the name [_n-gram_](https://en.wikipedia.org/wiki/N-gram) for a sequence of $n$ characters. Our method should work for any $n>0$.\n",
    "\n",
    "For example, take our initial text to be the 3 characters \"is \":\n",
    "There are 3 occurrences of this $n$-gram in the text. In this case, the next letter is \"e\" once and \"a\" twice, so we estimate the conditional distribution to be:\n",
    "- P(\"e\" after \"is \") = 1/3\n",
    "- P(\"a\" after \"is \") = 2/3\n",
    "\n",
    "So we pick randomly from this distribution. Say we pick \"e\". Then our output text is now \"is e\" but our current $n$-gram is just \"s e\" because we're only using $n=3$. So to pick the next character after this, we'd look at what happens after occurrences of \"s e\". And so on.\n",
    "\n",
    "In order to implement this idea efficiently, you will pre-compute the conditional probability distribution for every possible $n$ gram. To do that we need to count, for every possibly $n$-gram, the frequencies of the possible next characters, and then normalize them into probability distributions.\n",
    "\n",
    "*Attribution*: this exercise adapted with permission from Princeton COS 126, [_Markov Model of Natural Language_]( http://www.cs.princeton.edu/courses/archive/fall15/cos126/assignments/markov.html). Original assignment was developed by Bob Sedgewick and Kevin Wayne. If you are interested in more background info, you can take a look at the original version. The original paper by Shannon, [A Mathematical Theory of Communication](https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf), essentially created the field of information theory and is thought to be one of the best scientific papers ever written (in terms of both impact and readability)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a9e3cab-1eef-48d5-9f4b-e6e7d99f7002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grimms' Fairy Tales by Jacob and Wilhelm Grimm\n",
    "data_url = 'http://www.gutenberg.org/files/2591/2591-0.txt'\n",
    "corpus = urllib.request.urlopen(data_url).read().decode(\"utf-8\")\n",
    "\n",
    "# remove the first chunk of characters, which contains some header stuff\n",
    "corpus = corpus[2820:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a79eb6fe-1384-49f9-8aa0-645a86f114a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   THE STORY OF THE YOUTH WHO WENT FORTH TO LEARN WHAT FEAR WAS\n",
      "     KING GRISLY-BEARD\n",
      "     IRON HANS\n",
      "     CAT-SKIN\n",
      "     SNOW-WHITE AND ROSE-RED\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "THE BROTHERS GRIMM FAIRY TALES\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "TH\n"
     ]
    }
   ],
   "source": [
    "print(corpus[:200])  # print out the first 200 characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09e66fd-89f7-4969-9aa0-792fdc911c80",
   "metadata": {},
   "source": [
    "#### 6(a): implementation\n",
    "rubric={accuracy:10,quality:10}"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fbadce26-bc6d-40cc-8b97-cade5a1d5d18",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# BEGIN QUESTION\n",
    "name: q6_a\n",
    "points: 20\n",
    "manual: true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01902d92-3e6b-4e67-b2b6-b87dac0d6195",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "You will implement the above algorithm in a class called `MarkovModel`. Your class will have the following methods:\n",
    "\n",
    "- `__init__`, which is already implemented for you.\n",
    "- `fit`, which calculates and stores the _frequencies_ of all possible next characters given an $n$-gram. These frequencies should be stored in a `dict` of `dicts`, where the keys of the outer `dict` are the $n$-grams and the keys of the inner `dict` are the possible next characters, and the values of the inner `dict` are the frequencies (counts). Then, at the end of `fit`, normalize these frequencies into empirical probabilities and store them in `self.probabilities`.\n",
    "**Note:** before starting the calculations, append the first $n$ characters of your corpus to the end of the corpus, making it \"circular\"; this will avoid a situation where you your `generate` function might get stuck when your loop reaches the end of the corpus.\n",
    "- `generate`, which creates a random text of a specified length by generating one character at a time from the appropriate (discrete) probability distribution. To perform the random sampling, use the parameter `p=` of `np.random.choice`. You can start the output text with the first $n$ characters of the input text.\n",
    "\n",
    "**Note:** you may find some of the fancy dictionaries in the [`collections`](https://docs.python.org/3.7/library/collections.html) package useful, namely `defaultdict` and/or `Counter`. However, you can also just use `dict`; either way is fine.\n",
    "\n",
    "**Hint:** if you find yourself searching for all occurrences of an $n$-gram in the text, you are approaching this incorrectly - in that case, ask us for help!"
   ]
  },
  {
   "cell_type": "raw",
   "id": "046b5e01-eb2e-49b3-803d-8c645baece6d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# BEGIN SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c9482b8-ebbe-49db-a399-ebcd2a66c09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarkovModel:\n",
    "    \"\"\"A Markov model of languages based on character frequencies in text.\"\"\"\n",
    "\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "        self.probabilities = None\n",
    "        self.starting_chars = None\n",
    "\n",
    "    def fit(self, text):\n",
    "        \"\"\"\n",
    "        Fit a Markov model and create a transition matrix.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        text : str\n",
    "            a corpus of text \n",
    "        \"\"\"\n",
    "        \n",
    "        # store the first n characters of the training text, as we will use these\n",
    "        # to seed our `generate` function\n",
    "        self.starting_chars = text[:self.n]\n",
    "        \n",
    "        # make text circular so Markov chain doesn't get stuck\n",
    "        circ_text = text + text[:self.n]\n",
    "\n",
    "        # Step 1: Compute frequencies\n",
    "        # FILL IN THE REST OF THE CODE HERE\n",
    "        # BEGIN SOLUTION\n",
    "        # count the number of occurrences of each letter following a given n-gram\n",
    "        frequencies = defaultdict(Counter)\n",
    "        for i in range(len(text)): \n",
    "            ngram = circ_text[i:i+self.n]\n",
    "            next_char = circ_text[i+self.n]\n",
    "            frequencies[ngram][next_char] += 1\n",
    "        # END SOLUTION\n",
    "        \n",
    "        # Step 2: Normalize the frequencies into probabilities\n",
    "        # FILL IN THE REST OF THE CODE HERE\n",
    "        # BEGIN SOLUTION\n",
    "        self.probabilities = defaultdict(dict)\n",
    "        for ngram, counts in frequencies.items():\n",
    "            total_count = np.sum(list(counts.values()))\n",
    "            for next_char, count in counts.items():\n",
    "                self.probabilities[ngram][next_char] = count / total_count\n",
    "        # END SOLUTION\n",
    "        \n",
    "    def generate(self, seq_len):\n",
    "        \"\"\"\n",
    "        Generate a sequence of length seq_len, Markov model learned in `fit`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        seq_len : int\n",
    "            the desired length of the sequence\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            the generated sequence\n",
    "        \"\"\"\n",
    "        s = self.starting_chars\n",
    "        while len(s) < seq_len:\n",
    "            current_ngram = s[-self.n:]\n",
    "            \n",
    "            # FILL IN THE REST OF THE CODE HERE\n",
    "            # BEGIN SOLUTION\n",
    "            probs = self.probabilities[current_ngram]\n",
    "            s += np.random.choice(list(probs.keys()), p=list(probs.values()))\n",
    "            # END SOLUTION\n",
    "        return s"
   ]
  },
  {
   "cell_type": "raw",
   "id": "05697464-b001-461f-9091-0cfaf4292c11",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ae0cde-c777-4bc4-81d9-3f5b6589278d",
   "metadata": {},
   "source": [
    "Here are some tests that should pass if `fit` is implemented correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29e69e7d-494b-4431-8af0-1ec3c6e4ad02",
   "metadata": {},
   "outputs": [],
   "source": [
    "mm = MarkovModel(n=2)\n",
    "test_corpus = \"2 + 2 = 4; 2 + 3 = 5; 3 + 3 = 9; 3 + 2 = 5;\"\n",
    "mm.fit(test_corpus)\n",
    "\n",
    "assert mm.starting_chars == '2 '\n",
    "assert mm.probabilities['2 ']['+'] == 1/2\n",
    "assert mm.probabilities[' 3'][' '] == 1\n",
    "assert mm.probabilities[';2'][' '] == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "019051b1-e48c-4f55-ab19-1e2de2da8a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 + 2 = 9; 3 + 2 = 9; 3 + 2 + 3 + 2 + 2 \n"
     ]
    }
   ],
   "source": [
    "print(mm.generate(40))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a05d0e-c5cb-4257-8c3c-31054fa80346",
   "metadata": {},
   "source": [
    "And here we run it on our fairy tales corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d48a429-8d52-400b-802d-f07ab4b4339c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mm = MarkovModel(n=5)\n",
    "mm.fit(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07e0d214-b5bd-4895-bbce-12bc035ba4ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   THE HORSE\n",
      "\n",
      "\n",
      "\n",
      "The princess, they were soon learn it to a deep good\n",
      "so that the sea.\n",
      "When the children an unluckily seemed as to cut with her, and enjoy in the better.’ But they crowd\n",
      "plucked \n"
     ]
    }
   ],
   "source": [
    "print(mm.generate(200))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bcc1dbc0-edd3-4936-8b1b-2edfd6b2c632",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# END QUESTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1853ad-ec35-482e-8885-04a7ffb256c4",
   "metadata": {},
   "source": [
    "#### 6(b): fun with language models\n",
    "rubric={reasoning:5}"
   ]
  },
  {
   "cell_type": "raw",
   "id": "edcc250a-2680-4724-9e5a-a9dff5b179bf",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# BEGIN QUESTION\n",
    "name: q6_b\n",
    "points: 5\n",
    "manual: true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e48b13-002a-4d1c-9885-c94d3623c4b3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "1. Explain what happens as you increase $n$ from 1 to larger and larger values. At what point does it start to look like English? At what point is your model just memorizing the input corpus?\n",
    "\n",
    "2. Generate some random sequences using the data set of your choice. Submit your favourite randomly generated sequence as well as the link to the data you used to generate it. If you are out of ideas, you may find some text files of popular books [here](http://www.gutenberg.org/)."
   ]
  },
  {
   "cell_type": "raw",
   "id": "b24a1043-70fd-41d8-9fe7-7261330c10ba",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# BEGIN SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dad5157-f45d-489b-8f91-e6acadfd41c4",
   "metadata": {
    "tags": []
   },
   "source": [
    "1. As  $n$  gets larger the output makes more sense. Around  $n$=5  it looks like English. Around  $n$=15  it look like it's memorizing the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d0d0993-72dc-45d3-a57b-5092bea99ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   THE STORY OF THE YOUTH WHO WENT FORTH TO LEARN WHAT FEAR WAS\n",
      "     KING GRISLY-BEARD\n",
      "\n",
      "\n",
      "A great king of a land far away in time. Just as he was come round, they thought what had happened. ‘My stars!’ said he, ‘I have not bent one hair of mine.’ Then the fox\n",
      "and the little\n",
      "fish, and has a\n",
      "little pointed mouth?’ ‘Yes,’ said the other; ‘let us follow the\n",
      "prince thought it with me, and was as strong and well behaved, and scorned to ask what you can draw him to your grandmother is here, and \n"
     ]
    }
   ],
   "source": [
    "## IGNORE ##\n",
    "# 2.\n",
    "model = MarkovModel(n=10)\n",
    "model.fit(corpus)\n",
    "print(model.generate(500))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9f6499bf-eb3e-4223-8985-086d90a71674",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0595facd-011e-4647-b248-0c370b85c99a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# END QUESTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fdbe58-79c2-4c9e-8f25-70f6927ca045",
   "metadata": {},
   "source": [
    "#### (challenging) 6(c): time complexity of `fit`\n",
    "rubric={reasoning:1}"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a1b631a4-9b0b-4d57-b74b-6f6a31c0b6a7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# BEGIN QUESTION\n",
    "name: q6_c\n",
    "points: 1\n",
    "manual: true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23973c1a-70bc-43e1-ab04-4bc9dc29f04a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "For the above implementation, what is the (worst case) time complexity of running `fit` in terms of:\n",
    "\n",
    "- $n$, the length of each $n$-gram\n",
    "- the length of the corpus, which we'll call $N$\n",
    "- the length of the sequence to generate, `seq_len`, which we'll call $T$\n",
    "\n",
    "You can assume `np.random.choice` takes $O(1)$ time. You can also assume $n \\ll N$ and $n \\ll T$."
   ]
  },
  {
   "cell_type": "raw",
   "id": "bcb9ba8b-5207-4318-bdca-1a3bedc361f1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# BEGIN SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a148ba51-64df-4eda-89da-74d6aef12353",
   "metadata": {},
   "source": [
    "**Short answer / acceptable answer:** time complexity is $O(N)$ if we assume all the dictionary operations are $O(1)$ time. The number of different $n$-grams cannot be more than $N$, so we can neglect the second loop.\n",
    "\n",
    "**Optional assumption:** It is valid (but not required) to assume that inserting a key of size $k$ into a dictionary takes $O(k)$ time, because the key must be hashed, and hashing is a linear-time operation. This would yield $O(Nn)$ running time as we insert an $n$-sized ngram (at most) $N$ times in both loops.\n",
    "\n",
    "**Optional, more precise answer:** Although the above is accurate, we can obtain a more precise time bound based on $n$ in some circumstances. For example, if $n=1$ then you can only have at most $c$ keys in the outer dictionary, where $c$ is the number of possible characters in your alphabet. (Note that we're mainly concerned with the outer dictionary here, because the inner dictionaries can only have at most $c$ keys each, which we can ignore as a constant factor.) In general, the number of possible $n$-grams given an alphabet size of $c$ is $c^n$. So, if $N$ is huge such that you are able to see all those $c^n$ different $n$-grams in the corpus, then you can expect $O(c^n)$ to appear in the time complexity. However, once $c^n\\geq N$ then this effect disappears, because you cannot possibly have more than $N$ keys in your outer dictionary. Thus you could say the time complexity is $c^n$ if $c^n<N$ or $N$ otherwise.\n",
    "\n",
    "These piece-wise function can be succinctly written as $O(\\min(c^n,N))$, where the min function returns the smaller of the two inputs. This expression is still not quite right, because we always need to do a loop over the corpus, which takes $O(N)$ time. That is, even if $c^n < N$, such that the worst case number of keys is $c^n$, we still have an $O(N)$ step. We can capture all this by saying the running time is $O(\\min(c^n,N)+N)$. In other words, if $c^n < N$ then the running time is $O(c^n+N)$, and otherwise it's just $O(N)$. Finally, if we simply assume that $c$ is very large then we'll be in the $O(N)$ situation; in other words, we revert back to the \"short answer\" above.\n",
    "\n",
    "Also: these are worst case running times, which assume a very un-repetitive text. The typical case might be different and depends on the corpus. As an extreme example, if the corpus is all the same letter, like \"aaaaaaaaaaaa\", then the dictionary will only have 1 element in it and `fit` will run super fast."
   ]
  },
  {
   "cell_type": "raw",
   "id": "ff5f0b56-2465-40fb-9765-2dda0cf52657",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3b527025-3c2d-4da5-81b8-a525c26b16ae",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# END QUESTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0968e945-ca7a-4e7f-bf9c-942e92d74ae4",
   "metadata": {},
   "source": [
    "#### (challenging) Exercise 6d: time complexity of `generate`\n",
    "rubric={reasoning:0.5}"
   ]
  },
  {
   "cell_type": "raw",
   "id": "eadcb841-f31e-42a4-a0e0-cf9b701f9c98",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# BEGIN QUESTION\n",
    "name: q6_d\n",
    "points: 0.5\n",
    "manual: true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a082ec6-e79a-483d-9f2a-a841461eb261",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "For the above implementation, what is the (worst case) time complexity of running `generate` in terms of $n$, $N$, and $T$?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8d2ab48f-0c7a-45dc-9abd-03ec969b5566",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# BEGIN SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067a4bcf-5d59-4726-9f8b-e9dddfe92b42",
   "metadata": {},
   "source": [
    "**Short answer / expected answer:** for `generate`, the time complexity is $O(T)$. \n",
    "\n",
    "**Optional assumption:** It is valid (but not required) to assume that inserting/retrieving a key of size $k$ into a dictionary takes $O(k)$ time, because the key must be hashed, and hashing is a linear-time operation. This would yield $O(Tn)$ running time as we retrieve an $n$-sized ngram $T$ times.\n",
    "\n",
    "**Longer answer:** if one wants to worry about string concatenation taking time proportional to the length of the string, then it might actually be $O(T^2)$. However, there are still ways to implement this in $O(T)$ time by preallocating an array of length $T$ and then filling it in character by character. So, while the particular implementation above might be slower, $O(T)$ is easily achievable if necessary. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "b07e3898-cf17-429c-ad6c-44d557fba592",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1718e5f9-c249-427b-9735-d73874f7facf",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# END QUESTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462dc869-ffde-4b47-8ec5-616897b5d2c9",
   "metadata": {},
   "source": [
    "#### (challenging) Exercise 6(e): total time complexity\n",
    "rubric={reasoning:0.5}"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ce3fb90c-da61-40c1-b40d-2c681d09b576",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# BEGIN QUESTION\n",
    "name: q6_e\n",
    "points: 0.5\n",
    "manual: true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72981093-57f7-486e-9959-b47a635c96f6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "What is the total time complexity of running `fit` once and then `generate` once, in terms of $n$, $N$, and $T$?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b9584283-0354-462a-b718-7e641f289955",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# BEGIN SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9ca8fb-66db-4d29-9236-19db32671f5d",
   "metadata": {},
   "source": [
    "To answer this question we need to add the time complexities from the answers above. Using the short answers of $O(N)$ for `fit` and $O(T)$ for `generate`, the overall time complexity is $O(N+T)$.\n",
    "\n",
    "**Note:** Of course, if they provided one of the optional/longer answers to the questions above, then we should accept $O(F+G)$, where $F$ is their time complexity for `fit` and `g` is their time complexity for $G$."
   ]
  },
  {
   "cell_type": "raw",
   "id": "94364435-8a5f-4566-bc8e-cc19ec93fb03",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5ae9ce44-7c15-4fca-ba49-f25a3545f16d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# END QUESTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589fb352-fbb1-49f6-b3ca-0ad2bd7f9911",
   "metadata": {},
   "source": [
    "#### (challenging) 6(f): space complexity\n",
    "rubric={reasoning:0.5}"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ba4fd9f3-5a21-47d0-b14c-9f25872952f2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# BEGIN QUESTION\n",
    "name: q6_f\n",
    "points: 0.5\n",
    "manual: true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6a7b2f-1f03-490e-b199-ac2112f06215",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "What is the space complexity of `fit`?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "00fa2422-d100-4cdb-a545-7439c55cc99b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# BEGIN SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc13ec30-896f-4b0e-94f7-5f4bd5f9ac5a",
   "metadata": {},
   "source": [
    "`fit`: The short answer is that in the worst case we could have $N$ different $n$-grams in our dictionary, and each one takes up $O(n)$ storage, meaning a space complexity of $O(Nn)$. However, there is a longer story as with the parts above. $O(Nn)$ space complexity is only true if the $n$-grams are all different. We can try to bound the number by looking at the size of the alphabet, $c$. With only $c$ possible characters, we could only have $c^n$ possible $n$-grams, which may be smaller than $N$ if $c$ and/or $n$ is small. So we again have $O(\\min(c^n,N)n)$."
   ]
  },
  {
   "cell_type": "raw",
   "id": "9512d036-ef10-477e-945a-7482bbadc049",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "raw",
   "id": "894f6955-1720-428c-84ef-ccd3e14fbb22",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# END QUESTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42ef622-f21c-4ab3-ae9e-7c676a3b11ff",
   "metadata": {},
   "source": [
    "#### (challenging) 6(g): space complexity of `generate`\n",
    "rubric={reasoning:0.5}"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fad47732-fe63-4832-addb-c33bd564586e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# BEGIN QUESTION\n",
    "name: q6_g\n",
    "points: 0.5\n",
    "manual: true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1bca69-5cd8-466c-bb50-eddc70bf26e8",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "What is the space complexity of `generate`?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3e06705d-e9af-42b1-9a58-d6e93023c890",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# BEGIN SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd596d39-72de-4d77-a864-157eeb4222bc",
   "metadata": {},
   "source": [
    "`generate`: Space complexity is $O(T)$. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "a35960dc-557f-48ab-ad14-47fc63396497",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "raw",
   "id": "010f618b-bc57-4e94-810e-5371d4907160",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# END QUESTION"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
